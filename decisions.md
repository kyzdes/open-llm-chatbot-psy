# Лог решений — FreePsy

## Решение 1: Python + aiogram 3.x
**Дата**: 2026-02-12
**Контекст**: Выбор языка и фреймворка для Telegram-бота
**Решение**: Python с aiogram 3.x
**Обоснование**: Самая богатая экосистема для Telegram-ботов, нативная асинхронность, FSM из коробки

## Решение 2: SQLite + aiosqlite для персистентности
**Дата**: 2026-02-12
**Контекст**: Нужна полная история диалогов между сессиями
**Решение**: SQLite с WAL mode через aiosqlite
**Обоснование**: Нет внешних зависимостей, достаточно для MVP, WAL mode обеспечивает конкурентный доступ

## Решение 3: DeepSeek R1 как модель по умолчанию
**Дата**: 2026-02-12
**Контекст**: Нужна бесплатная модель с максимальным контекстом и качеством
**Решение**: `deepseek/deepseek-r1-0528:free` (671B, 164K контекст)
**Обоснование**: Самая мощная бесплатная модель на OpenRouter. 671B параметров и 164K контекстное окно позволяют вести длинные терапевтические диалоги

## Решение 4: Двухслойная детекция кризисов
**Дата**: 2026-02-12
**Контекст**: Безопасность пользователей — критически важный аспект
**Решение**: Слой 1 (ключевые слова, мгновенный) + Слой 2 (LLM, точный)
**Обоснование**: Ключевые слова дают мгновенную реакцию без задержки API. LLM даёт более точную оценку контекста

## Решение 5: Скользящее окно 100K токенов
**Дата**: 2026-02-12
**Контекст**: Модель имеет 164K контекст, нужно оставить место для ответа и системного промпта
**Решение**: Бюджет ~100K токенов для истории, оценка через `len(text)/3`
**Обоснование**: 100K из 164K оставляет ~64K для системного промпта и генерации ответа. Деление на 3 — разумная оценка для русского текста (UTF-8, ~3 байта на символ)

## Решение 6: Typing keep-alive каждые 4 секунды
**Дата**: 2026-02-12
**Контекст**: DeepSeek R1 думает 30-60 секунд, typing indicator Telegram живёт 5 секунд
**Решение**: Фоновая задача отправляет typing action каждые 4 секунды
**Обоснование**: Пользователь видит что бот «печатает» всё время ожидания

## Решение 7: Admin-only `/modelchange`
**Дата**: 2026-02-12
**Контекст**: Нужна возможность менять модель для всех пользователей
**Решение**: Команда `/modelchange model_name` доступна только пользователю с ID 83979215
**Обоснование**: Простой и безопасный механизм управления моделью

## Решение 8: Strip `<think>` блоков из ответа
**Дата**: 2026-02-12
**Контекст**: DeepSeek R1 генерирует reasoning tokens в `<think>` тегах
**Решение**: `include_reasoning: false` + regex strip `<think>...</think>`
**Обоснование**: Пользователю не нужно видеть внутренний процесс рассуждений модели

## Решение 9: Token Bucket rate limiting (in-memory)
**Дата**: 2026-02-12
**Контекст**: Защита от абьюза LLM API
**Решение**: Token Bucket (3 burst, 1/10s), in-memory per user
**Обоснование**: Простая реализация, достаточная для MVP. Сбрасывается при рестарте — приемлемо

## Решение 10: therapy.router регистрируется последним
**Дата**: 2026-02-12
**Контекст**: Handler для текстовых сообщений — catch-all
**Решение**: Роутер therapy включается последним при регистрации
**Обоснование**: Catch-all не должен перехватывать команды и другие хендлеры

## Решение 11: HTML вместо Markdown для форматирования
**Дата**: 2026-02-13
**Контекст**: `parse_mode="Markdown"` (legacy) не поддерживает `##`, вложенные списки, ломается на незакрытых `*`/`_`/`` ` ``
**Решение**: Переход на `parse_mode="HTML"` везде. LLM-ответы конвертируются из Markdown в HTML через `md_to_html()` + `sanitize_html()`
**Обоснование**: HTML — надёжнее для Telegram, не ломается от LLM-разметки. Fallback на plain text при ошибке парсинга. Чанки нарезаются до конвертации (≤3500 символов raw) чтобы не разрезать теги

## Решение 12: Жёсткое ограничение краткости в SYSTEM_PROMPT
**Дата**: 2026-02-13
**Контекст**: Ответы LLM были слишком длинными, правило краткости стояло на 10-м месте и было мягким
**Решение**: Правило краткости — на 1-е место с жёстким лимитом (3-5 предложений, макс 2-3 абзаца), запрет списков и заголовков, дублирующее напоминание в конце промпта
**Обоснование**: Пользователи читают с телефона, длинные ответы не читаются. Запрет списков вынуждает модель писать живым текстом

## Решение 13: Меню команд через set_my_commands()
**Дата**: 2026-02-13
**Контекст**: Пользователи не знали о доступных командах, приходилось вводить вручную
**Решение**: `bot.set_my_commands()` при запуске с 7 пользовательскими командами. Админские команды не включены. Обёрнуто в try/except
**Обоснование**: Telegram показывает кнопку `/` → список команд. Не влияет на запуск при сбое API

## Решение 14: Динамический список моделей вместо хардкода
**Дата**: 2026-02-13
**Контекст**: Для смены модели нужно было знать точный API ID, список моделей менялся
**Решение**: `GET /api/v1/models` → фильтрация бесплатных → inline-клавиатура. Кеш 10 минут. callback_data через индекс (не ID модели — защита от лимита 64 байта)
**Обоснование**: Пользователь видит актуальный список, выбирает кнопкой. Кеш снижает нагрузку на API. Валидация пробным запросом перед сохранением
